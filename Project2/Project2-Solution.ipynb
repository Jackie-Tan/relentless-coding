{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 2\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "---\n",
    "\n",
    "Your hometown mayor just created a new data analysis team to give policy advice, and the administration recruited _you_ via LinkedIn to join it. Unfortunately, due to budget constraints, for now the \"team\" is just you...\n",
    "\n",
    "The mayor wants to start a new initiative to move the needle on one of two separate issues: high school education outcomes, or drug abuse in the community.\n",
    "\n",
    "Also unfortunately, that is the entirety of what you've been told. And the mayor just went on a lobbyist-funded fact-finding trip in the Bahamas. In the meantime, you got your hands on two national datasets: one on SAT scores by state, and one on drug use by age. Start exploring these to look for useful patterns and possible hypotheses!\n",
    "\n",
    "---\n",
    "\n",
    "This project is focused on exploratory data analysis, aka \"EDA\". EDA is an essential part of the data science analysis pipeline. Failure to perform EDA before modeling is almost guaranteed to lead to bad models and faulty conclusions. What you do in this project are good practices for all projects going forward, especially those after this bootcamp!\n",
    "\n",
    "This lab includes a variety of plotting problems. Much of the plotting code will be left up to you to find either in the lecture notes, or if not there, online. There are massive amounts of code snippets either in documentation or sites like [Stack Overflow](https://stackoverflow.com/search?q=%5Bpython%5D+seaborn) that have almost certainly done what you are trying to do.\n",
    "\n",
    "**Get used to googling for code!** You will use it every single day as a data scientist, especially for visualization and plotting.\n",
    "\n",
    "#### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# this line tells jupyter notebook to put the plots in the notebook rather than saving them to file.\n",
    "%matplotlib inline\n",
    "\n",
    "# this line makes plots prettier on mac retina screens. If you don't have one it shouldn't do anything.\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 1. Load the `sat_scores.csv` dataset and describe it\n",
    "\n",
    "---\n",
    "\n",
    "You should replace the placeholder path to the `sat_scores.csv` dataset below with your specific path to the file.\n",
    "\n",
    "### 1.1 Load the file with the `csv` module and put it in a Python dictionary\n",
    "\n",
    "The dictionary format for data will be the column names as key, and the data under each column as the values.\n",
    "\n",
    "Toy example:\n",
    "```python\n",
    "data = {\n",
    "    'column1':[0,1,2,3],\n",
    "    'column2':['a','b','c','d']\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_file = 'sat_scores.csv'\n",
    "sat_dict = {}\n",
    "with open(sat_file, 'r') as csv_file:\n",
    "    # Using csv to read file\n",
    "    sat_reader = csv.reader(csv_file)\n",
    "    # To track if it's header or data rows\n",
    "    row_count = 0\n",
    "    for row in sat_reader:\n",
    "        # Header row\n",
    "        if row_count == 0:\n",
    "            # Convert the header row into Dict keys\n",
    "            sat_dict = sat_dict.fromkeys(row, [])\n",
    "        # Data row\n",
    "        else:\n",
    "            for column, name in enumerate(sat_dict):\n",
    "                # Add each element in a row to its associated keys in the Dict\n",
    "                temp = sat_dict[name] + [row[column]]\n",
    "                sat_dict[name] = temp\n",
    "        row_count += 1\n",
    "    csv_file.close()\n",
    "sat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach to read csv\n",
    "# raw_pd = ''\n",
    "# sat_file = 'sat_scores.csv'\n",
    "# with open(sat_file, 'r') as csvfile:\n",
    "#     raw_pd = csvfile.read()\n",
    "    \n",
    "# raw_pd = raw_pd.splitlines()\n",
    "# header = raw_pd[0].split(',')\n",
    "# data = [data.split(',') for data in raw_pd[1:]]\n",
    "# sat_dict = {head:[row[idx] for row in data] for idx, head in enumerate(header)}\n",
    "# sat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Make a pandas DataFrame object with the SAT dictionary, and another with the pandas `.read_csv()` function\n",
    "\n",
    "Compare the DataFrames using the `.dtypes` attribute in the DataFrame objects. What is the difference between loading from file and inputting this dictionary (if any)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_to_pandas = pd.DataFrame(sat_dict)\n",
    "print(raw_to_pandas.dtypes)\n",
    "read_by_pandas = pd.read_csv(sat_file)\n",
    "print(read_by_pandas.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color:black;color:white;padding:5px'>\n",
    "<h3 style='margin:3px'>Yes, the difference is that for the raw data, they are all read as string type, whereas Pandas will convert numeric values to integer types whenever possible.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not convert the string column values to float in your dictionary, the columns in the DataFrame are of type `object` (which are string values, essentially). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Look at the first ten rows of the DataFrame: what does our data describe?\n",
    "\n",
    "From now on, use the DataFrame loaded from the file using the `.read_csv()` function.\n",
    "\n",
    "Use the `.head(num)` built-in DataFrame function, where `num` is the number of rows to print out.\n",
    "\n",
    "You are not given a \"codebook\" with this data, so you will have to make some (very minor) inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_by_pandas.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color:black;color:white;padding:5px;'>\n",
    "<h3 style='margin:3px;color:white;'>\n",
    "    'State': Probably all states in USA <br>\n",
    "    'Rate': Participation rate by state <br>\n",
    "    'Verbal': Average verbal score by state <br>\n",
    "    'Math': Average math score by state\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store and remove the last row of the data. It contains some information of all the states.\n",
    "total_row = read_by_pandas.iloc[-1:]\n",
    "read_by_pandas = read_by_pandas.iloc[:-1]\n",
    "read_by_pandas.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 2. Create a \"data dictionary\" based on the data\n",
    "\n",
    "---\n",
    "\n",
    "A data dictionary is an object that describes your data. This should contain the name of each variable (column), the type of the variable, your description of what the variable is, and the shape (rows and columns) of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {column:{'type':read_by_pandas.dtypes[column]} for column in read_by_pandas.columns}\n",
    "data_dict.update({'shape':read_by_pandas.shape})\n",
    "data_dict['State'].update({'description':'Abbreviation of states in USA'})\n",
    "data_dict['Rate'].update({'description':'Participation rate by state'})\n",
    "data_dict['Math'].update({'description':'Average mathematics score by state'})\n",
    "data_dict['Verbal'].update({'description':'Average verbal score by state'})\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 3. Plot the data using seaborn\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Using seaborn's `distplot`, plot the distributions for each of `Rate`, `Math`, and `Verbal`\n",
    "\n",
    "Set the keyword argument `kde=False`. This way you can actually see the counts within bins. You can adjust the number of bins to your liking. \n",
    "\n",
    "[Please read over the `distplot` documentation to learn about the arguments and fine-tune your chart if you want.](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.distplot.html#seaborn.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as axes\n",
    "import seaborn as sns\n",
    "sns.set(style = 'darkgrid')\n",
    "# Plot the distribution for Rate\n",
    "plt.figure(figsize=(15,5))\n",
    "plot = sns.distplot(read_by_pandas['Rate'], kde=False, bins=8, color='red')\n",
    "plot.set(ylabel='Frequency', title='Participation Rate')\n",
    "for rect in plot.patches:\n",
    "    plot.text(s = int(rect.get_height()), y = rect.get_height(), x = rect.get_x() + rect.get_width() / 2)\n",
    "plt.show()\n",
    "# Plot the distribution for Math\n",
    "plt.figure(figsize=(15,5))\n",
    "plot = sns.distplot(read_by_pandas['Math'], kde=False, bins=10, color='green', axlabel='Score');\n",
    "plot.set(ylabel='Frequency', title='Math Score')\n",
    "for rect in plot.patches:\n",
    "    plot.text(s = int(rect.get_height()), y = rect.get_height(), x = rect.get_x() + rect.get_width() / 2)\n",
    "plt.show()\n",
    "# Plot the distribution for Verbal\n",
    "plt.figure(figsize=(15,5))\n",
    "plot = sns.distplot(read_by_pandas['Verbal'], kde=False, bins=10, color='orange', axlabel='Score');\n",
    "plot.set(ylabel='Frequency', title='Verbal Score')\n",
    "for rect in plot.patches:\n",
    "    plot.text(s = int(rect.get_height()), y = rect.get_height(), x = rect.get_x() + rect.get_width() / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using seaborn's `pairplot`, show the joint distributions for each of `Rate`, `Math`, and `Verbal`\n",
    "\n",
    "Explain what the visualization tells you about your data.\n",
    "\n",
    "[Please read over the `pairplot` documentation to fine-tune your chart.](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.pairplot.html#seaborn.pairplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(read_by_pandas, height = 4, kind='reg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">Negative correlations are observed between \"Rate\" and \"Math\", and between \"Rate\" and \"Verbal\", whereas positive correlation is observed between \"Math\" and \"Verbal\". <br><br> The strength of the correlation between \"Math\" and \"Verbal\" appears stronger than the other two correlations.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 4. Plot the data using built-in pandas functions.\n",
    "\n",
    "---\n",
    "\n",
    "Pandas is very powerful and contains a variety of nice, built-in plotting functions for your data. Read the documentation here to understand the capabilities:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "\n",
    "### 4.1 Plot a stacked histogram with `Verbal` and `Math` using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = read_by_pandas[['Verbal', 'Math']].plot.hist(stacked=True, figsize=(15,5));\n",
    "ax.set(xlabel = 'Score', title='Verbal and Math Scores (Stacked)');\n",
    "for rect in ax.patches:\n",
    "    if rect.get_height() > 0:\n",
    "        ax.text(s = int(rect.get_height()), y = rect.get_y() + rect.get_height() - 1, x = rect.get_x() + rect.get_width() / 2)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Plot `Verbal` and `Math` on the same chart using boxplots\n",
    "\n",
    "What are the benefits of using a boxplot as compared to a scatterplot or a histogram?\n",
    "\n",
    "What's wrong with plotting a box-plot of `Rate` on the same chart as `Math` and `Verbal`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, bp = read_by_pandas[['Verbal', 'Math']].boxplot(return_type='both');\n",
    "ax.set(ylabel = 'Score');\n",
    "# [box.get_ydata() for box in bp[\"boxes\"]]\n",
    "# for i in bp:\n",
    "#     for j in bp[i]:\n",
    "#         print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">Benefits are a quick glance between multiple variables so that we can easily compared their mean, range, interquartile range (IQR) and outliers, if any.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">Rate is a measure in percentage, ranged between 0 to 100, whereas Math and Verbal are absolute values with higher and wider ranges. Hence, by plotting a box-plot of Rate on the same chart as Math and Verbal, the comparsion will appear off and meaningless. </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 4.3 Plot `Verbal`, `Math`, and `Rate` appropriately on the same boxplot chart\n",
    "\n",
    "Think about how you might change the variables so that they would make sense on the same chart. Explain your rationale for the choices on the chart. You should strive to make the chart as intuitive as possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_by_pandas[['Rate','Math','Verbal']].apply(lambda x: (x-x.min())/(x.max()-x.min())).boxplot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">Using feature scaling like Max-Min normalization, all 3 variables will be subjected within the range of 0 and 1. The distance between each data points is now comparable and the values can also be used in machine learning algorithms. </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 5. Create and examine subsets of the data\n",
    "\n",
    "---\n",
    "\n",
    "For these questions you will practice **masking** in pandas. Masking uses conditional statements to select portions of your DataFrame (through boolean operations under the hood.)\n",
    "\n",
    "Remember the distinction between DataFrame indexing functions in pandas:\n",
    "\n",
    "    .iloc[row, col] : row and column are specified by index, which are integers\n",
    "    .loc[row, col]  : row and column are specified by string \"labels\" (boolean arrays are allowed; useful for rows)\n",
    "    .ix[row, col]   : row and column indexers can be a mix of labels and integer indices\n",
    "    \n",
    "For detailed reference and tutorial make sure to read over the pandas documentation:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
    "\n",
    "\n",
    "\n",
    "### 5.1 Find the list of states that have `Verbal` scores greater than the average of `Verbal` scores across states\n",
    "\n",
    "How many states are above the mean? What does this tell you about the distribution of `Verbal` scores?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbal_above_mean = read_by_pandas[read_by_pandas['Verbal'] > np.mean(read_by_pandas['Verbal'])]['State']\n",
    "print('Number of states:', len(verbal_above_mean))\n",
    "verbal_above_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">This tells me that since there are 24 states above the mean (balance point) and 27 states below or equal to the mean, which both of them have about the same size, the distribution is likely to be symmetrical balance. </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Find the list of states that have `Verbal` scores greater than the median of `Verbal` scores across states\n",
    "\n",
    "How does this compare to the list of states greater than the mean of `Verbal` scores? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbal_above_median = read_by_pandas[read_by_pandas['Verbal'] > np.median(read_by_pandas['Verbal'])]['State']\n",
    "print('Number of states:', len(verbal_above_median))\n",
    "verbal_above_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">The number of states that have verbal scores greater than the median of Verbal scores is equal to the number of states that are greater than the mean of Verbal scores. This reinforces the fact that the distribution is highly likely to be symmetrical. </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create a column that is the difference between the `Verbal` and `Math` scores\n",
    "\n",
    "Specifically, this should be `Verbal - Math`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_sat_df = read_by_pandas.assign(Diff=(read_by_pandas['Verbal'] - read_by_pandas['Math']))\n",
    "updated_sat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Create two new DataFrames showing states with the greatest difference between scores\n",
    "\n",
    "1. Your first DataFrame should be the 10 states with the greatest gap between `Verbal` and `Math` scores where `Verbal` is greater than `Math`. It should be sorted appropriately to show the ranking of states.\n",
    "2. Your second DataFrame will be the inverse: states with the greatest gap between `Verbal` and `Math` such that `Math` is greater than `Verbal`. Again, this should be sorted appropriately to show rank.\n",
    "3. Print the header of both variables, only showing the top 3 states in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df = updated_sat_df.sort_values('Diff', ascending=False).head(10)\n",
    "second_df = updated_sat_df.sort_values('Diff').head(10)\n",
    "print(first_df.head(3))\n",
    "print()\n",
    "print(second_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examine summary statistics\n",
    "\n",
    "---\n",
    "\n",
    "Checking the summary statistics for data is an essential step in the EDA process!\n",
    "\n",
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.1 Create the correlation matrix of your variables (excluding `State`).\n",
    "\n",
    "What does the correlation matrix tell you?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_by_pandas[['Rate','Math','Verbal']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">Negative correlations are observed between \"Rate\" and \"Math\", and between \"Rate\" and \"Verbal\", whereas positive correlation is observed between \"Math\" and \"Verbal\". <br><br> The strength of the correlation between \"Math\" and \"Verbal\" is the strongest among the 3 correlations.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.2 Use pandas'  `.describe()` built-in function on your DataFrame\n",
    "\n",
    "Write up what each of the rows returned by the function indicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_by_pandas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">\n",
    "        count: number of data points <br>\n",
    "        mean: average of each column <br>\n",
    "        std: standard deviation of each column <br>\n",
    "        min: minimum value of each column <br>\n",
    "        25%:1st quartile (Q1) of each column <br>\n",
    "        50%: median of each column <br>\n",
    "        75%: 3rd quartile (Q3) of each column <br>\n",
    "        max: maximum value of each column\n",
    "    </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.3 Assign and print the _covariance_ matrix for the dataset\n",
    "\n",
    "1. Describe how the covariance matrix is different from the correlation matrix.\n",
    "2. What is the process to convert the covariance into the correlation?\n",
    "3. Why is the correlation matrix preferred to the covariance matrix for examining relationships in your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the built-in function\n",
    "read_by_pandas[['Rate','Math','Verbal']].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">\n",
    "        The absolute values in a covariance matrix are much larger than that in a correlation matrix. Where the values in a correlation matrix are bounded between -1 and 1, the values in a covariance matrix are not bounded. \n",
    "    </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the process to convert the covariance into the correlation?\n",
    "cov_matrix = read_by_pandas[['Rate','Math','Verbal']].cov()\n",
    "std_dev = read_by_pandas[['Rate','Math','Verbal']].std()\n",
    "corr_matrix = cov_matrix.apply(lambda x: x/(std_dev[x.name]*std_dev[x.index]))\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">\n",
    "        To convert the covariance matrix into correlation matrix, simply divide the matrix by the standard deviations of the variables corresponding to the row index and the column index of the covariance matrix. <br>\n",
    "        \n",
    "\\begin{equation}\n",
    "Correlation = \n",
    " \\begin{pmatrix}\n",
    "  \\frac{Cov(1,1)}{\\sigma(1) \\times \\sigma(1)} & \\frac{Cov(1,2)}{\\sigma(1) \\times \\sigma(2)} & \\cdots & \\frac{Cov(1,n)}{\\sigma(1) \\times \\sigma(n)} \\\\\n",
    "  \\frac{Cov(2,1)}{\\sigma(2) \\times \\sigma(1)} & \\frac{Cov(2,2)}{\\sigma(2) \\times \\sigma(2)} & \\cdots & \\frac{Cov(2,n)}{\\sigma(2) \\times \\sigma(n)} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  \\frac{Cov(n,1)}{\\sigma(n) \\times \\sigma(1)} & \\frac{Cov(n,2)}{\\sigma(n) \\times \\sigma(2)} & \\cdots & \\frac{Cov(n,n)}{\\sigma(n) \\times \\sigma(n)} \n",
    " \\end{pmatrix}\n",
    " \\end{equation}\n",
    " \n",
    " \\begin{equation}\n",
    "Correlation = \n",
    " \\begin{pmatrix}\n",
    "  1 & \\frac{Cov(1,2)}{\\sigma(1) \\times \\sigma(2)} & \\cdots & \\frac{Cov(1,n)}{\\sigma(1) \\times \\sigma(n)} \\\\\n",
    "  \\frac{Cov(2,1)}{\\sigma(2) \\times \\sigma(1)} & 1 & \\cdots & \\frac{Cov(2,n)}{\\sigma(2) \\times \\sigma(n)} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  \\frac{Cov(n,1)}{\\sigma(n) \\times \\sigma(1)} & \\frac{Cov(n,2)}{\\sigma(n) \\times \\sigma(2)} & \\cdots & 1 \n",
    " \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "   </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 7. Performing EDA on \"drug use by age\" data.\n",
    "\n",
    "---\n",
    "\n",
    "You will now switch datasets to one with many more variables. This section of the project is more open-ended - use the techniques you practiced above!\n",
    "\n",
    "We'll work with the \"drug-use-by-age.csv\" data, sourced from and described here: https://github.com/fivethirtyeight/data/tree/master/drug-use-by-age.\n",
    "\n",
    "### 7.1\n",
    "\n",
    "Load the data using pandas. Does this data require cleaning? Are variables missing? How will this affect your approach to EDA on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_use_df = pd.read_csv('drug-use-by-age.csv')\n",
    "drug_use_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_use_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_use_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in drug_use_df:\n",
    "    print(drug_use_df[column].unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">\n",
    "Yes, this data requires cleaning. There are missing variables, and there is a need to change some values before EDA. For instance, alcohol-frequency is float type, but oxycontin-frequency is object type (likely string)\n",
    "    </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Do a high-level, initial overview of the data\n",
    "\n",
    "Get a feel for what this dataset is all about.\n",
    "\n",
    "Use whichever techniques you'd like, including those from the SAT dataset EDA. The final response to this question should be a written description of what you infer about the dataset.\n",
    "\n",
    "Some things to consider doing:\n",
    "\n",
    "- Look for relationships between variables and subsets of those variables' values\n",
    "- Derive new features from the ones available to help your analysis\n",
    "- Visualize everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_use_w_nan = drug_use_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns, except age and n, into float type\n",
    "# If not possible to convert, use NaN instead\n",
    "def convert_float(value):\n",
    "    \"\"\"\n",
    "    Attempt to convert inputs to float type, and if there is error converting, return NaN.\n",
    "    Paramters:\n",
    "    value: input value\n",
    "    Return:\n",
    "    float type of parameter value, or np.nan if conversion is impossible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return np.nan\n",
    "drug_use_w_nan.iloc[:,2:] = drug_use_w_nan.iloc[:,2:].applymap(convert_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To check if the columns have been converted to float type, if possible. If not, convert it to NaN.\n",
    "drug_use_w_nan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_use_w_nan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = [x for x in drug_use_w_nan.columns if 'frequency' in x]\n",
    "use = [x for x in drug_use_w_nan.columns if 'use' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between different drugs frequencies\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.heatmap(drug_use_w_nan[frequency].corr(), center = 0, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between different drugs uses\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.heatmap(drug_use_w_nan[use].corr(), center = 0, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_use_1 = pd.get_dummies(drug_use_w_nan, columns=['age'])\n",
    "drug_use_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between age group and drug frequencies, should someone in the age group abused the said drug\n",
    "frequency_age = [x for x in drug_use_1.columns if 'frequency' in x or 'age' in x]\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.heatmap(drug_use_1[frequency_age].corr(), center = 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between age group and drug use\n",
    "use_age = [x for x in drug_use_1.columns if 'use' in x or 'age' in x]\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.heatmap(drug_use_1[use_age].corr(), center=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the age band for which it has the highest frequency for each type of drug\n",
    "drug_use_idx_age = drug_use_w_nan.set_index('age')\n",
    "drug_use_idx_age[frequency].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the age band for which it has the highest uses for each type of drug\n",
    "drug_use_idx_age[use].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px; margin:10px;\">\n",
    "    <h3>My Inference on the Dataset</h3>\n",
    "    <p> The first heatmap is the correlation between all the drug frequencies, should someone is currently abusing the drugs. </p>\n",
    "    <p> It suggests that there are strong positive correlations between the frequency use of alcohol and marijuana, of cocaine and crack, of hallucinogen and inhalant, of stimulant and cocaine and of stimulant and crack. </p>\n",
    "    <p> It also suggests that there are strong negative correlations between the frequency use of hallucinogen and marijuana, of inhalant and marijuana, of tranquilizer and alcohol and of tranquilizer and marijuana. </p><br>\n",
    "    <p> The second heatmap is the correlation between all the drug uses. </p>\n",
    "    <p> The most prominent result is that inhalant use has strong negative correlations, or at most weak positive correlations, with all the other drug use. </p>\n",
    "    <p> Another result is that drug uses from pain releiver through meth has strong positive correlations among each other. </p><br>\n",
    "    <p> The age band is further labelled using dummies variables and the result dataset is once again used to measure correlation between each drug frequencies/uses and each age band. \n",
    "    <p> Interestingly, the third heatmap (frequencies only) suggests that drug abusers above 65 has the strongest correlation to many drug frequencies. However, the fourth heatmap (uses only) suggest that there are strong negative correlations between participants above 65 years old and the proportion of them using different drugs.</p>\n",
    "    <p>The fourth heatmap also suggest that for younger drug abusers, the correlation between them and the proportion of using inhalant drug is positively strongest among all drugs. This may suggest that inhalant may be much easier to be abused by younger drug abusers due to high accesibility and lower cost.</p>\n",
    "    <p> The last result shows that participants below 30 has the highest proportion of drug abusers for each drug mentioned in the survey. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Create a testable hypothesis about this data\n",
    "\n",
    "Requirements for the question:\n",
    "\n",
    "1. Write a specific question you would like to answer with the data (that can be accomplished with EDA).\n",
    "2. Write a description of the \"deliverables\": what will you report after testing/examining your hypothesis?\n",
    "3. Use EDA techniques of your choice, numeric and/or visual, to look into your question.\n",
    "4. Write up your report on what you have found regarding the hypothesis about the data you came up with.\n",
    "\n",
    "\n",
    "Your hypothesis could be on:\n",
    "\n",
    "- Difference of group means\n",
    "- Correlations between variables\n",
    "- Anything else you think is interesting, testable, and meaningful!\n",
    "\n",
    "**Important notes:**\n",
    "\n",
    "You should be only doing EDA _relevant to your question_ here. It is easy to go down rabbit holes trying to look at every facet of your data, and so we want you to get in the practice of specifying a hypothesis you are interested in first and scoping your work to specifically answer that question.\n",
    "\n",
    "Some of you may want to jump ahead to \"modeling\" data to answer your question. This is a topic addressed in the next project and **you should not do this for this project.** We specifically want you to not do modeling to emphasize the importance of performing EDA _before_ you jump to statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question and deliverables**\n",
    "\n",
    "Question: Is there any association between age bands and type of drugs (alcohol and crack) used? \n",
    "\n",
    "Deliverables: p-value and determine if there is any statistically significance between the 2 categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "alcohol_num = (drug_use_w_nan['alcohol-use'] / 100 * drug_use_w_nan['n']).apply(int)\n",
    "crack_num = (drug_use_w_nan['crack-use'] / 100 * drug_use_w_nan['n']).apply(int)\n",
    "alcohol_crack_df = pd.DataFrame([alcohol_num, crack_num]).T\n",
    "alcohol_crack_df.index = drug_use_w_nan['age']\n",
    "alcohol_crack_df.columns = ['# of alcohol abusers', '# of crack abusers']\n",
    "chi2, p, dof, expected = chi2_contingency(alcohol_crack_df)\n",
    "print('p-value:', p)\n",
    "alcohol_crack_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "# from scipy.stats import kruskal\n",
    "# import math\n",
    "# median_list = [[i] for i in drug_use_w_nan['crack-frequency'] if math.isnan(i) == False]\n",
    "# kruskal(*median_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report**\n",
    "\n",
    "Since the p-value is less than 0.05, there is evidence showing it is statistically significant that there is an association between the age band and the choice of drug (alcohol and crack) abused. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 8. Introduction to dealing with outliers\n",
    "\n",
    "---\n",
    "\n",
    "Outliers are an interesting problem in statistics, in that there is not an agreed upon best way to define them. Subjectivity in selecting and analyzing data is a problem that will recur throughout the course.\n",
    "\n",
    "1. Pull out the rate variable from the sat dataset.\n",
    "2. Are there outliers in the dataset? Define, in words, how you _numerically define outliers._\n",
    "3. Print out the outliers in the dataset.\n",
    "4. Remove the outliers from the dataset.\n",
    "5. Compare the mean, median, and standard deviation of the \"cleaned\" data without outliers to the original. What is different about them and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rate = read_by_pandas[['Rate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define outliers as values less than (mean - 1.5 * standard deviation) or values more than (mean + 1.5 * standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_mean = np.mean(original_rate['Rate'])\n",
    "original_median = np.median(original_rate['Rate'])\n",
    "original_std = np.std(original_rate['Rate'])\n",
    "outliers = original_rate[(original_rate['Rate'] < original_mean - 1.5 * original_std) | (original_rate['Rate'] > original_mean + 1.5 * original_std)]\n",
    "print(outliers)\n",
    "cleaned_rate = original_rate.drop(outliers.index)\n",
    "cleaned_mean = np.mean(cleaned_rate['Rate'])\n",
    "cleaned_median = np.median(cleaned_rate['Rate'])\n",
    "cleaned_std = np.std(cleaned_rate['Rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Mean:', original_mean)\n",
    "print('Original Median:', original_median)\n",
    "print('Original Standard Deviation:', original_std)\n",
    "print()\n",
    "print('Cleaned Mean:', cleaned_mean)\n",
    "print('Cleaned Median:', cleaned_median)\n",
    "print('Cleaned Standard Deviation:', cleaned_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">\n",
    "The mean, median and standard deviation have been reduced after removing the outliers. This is because given there are only 51 data points, which the dataset is relatively small, the outliers have the skew effect to pull the statistics towards their directions (positive direction). Hence, by removing them, this skew effect will be reduced. \n",
    "    </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 9. Percentile scoring and spearman rank correlation\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1 Calculate the spearman correlation of sat `Verbal` and `Math`\n",
    "\n",
    "1. How does the spearman correlation compare to the pearson correlation? \n",
    "2. Describe clearly in words the process of calculating the spearman rank correlation.\n",
    "  - Hint: the word \"rank\" is in the name of the process for a reason!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_by_pandas[['Math','Verbal']].corr('pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_by_pandas[['Math','Verbal']].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">\n",
    "        1. Spearman correlation gives a higher value compared to pearson correlation.\n",
    "    </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px;margin:10px;\">\n",
    "    <p> 1. Assign ranks to each value in each column (Math, Verbal), with rank 1 being the highest score in the respective column, followed by 2, 3, etc, and append the respective rank columns to each column (Math, Verbal). Sort the column first before assigning the rank number will be much easier! <p>\n",
    "    <p> 2. Take the difference between the ranks of Math column and Verbal column and square it. The column shall be $d^2$. </p>\n",
    "    <p> 3. Assume no tie in the rank columns, use this formula $$\\rho = 1 - \\frac{6\\sum_{i} d_i^2}{n(n^2 - 1)}$$, where $n = \\text{number of data points}$, to calculate the spearman rank correlation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Percentile scoring\n",
    "\n",
    "Look up percentile scoring of data. In other words, the conversion of numeric data to their equivalent percentile scores.\n",
    "\n",
    "http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html\n",
    "\n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.percentileofscore.html\n",
    "\n",
    "1. Convert `Rate` to percentiles in the sat scores as a new column.\n",
    "2. Show the percentile of California in `Rate`.\n",
    "3. How is percentile related to the spearman rank correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = pd.Series([stats.percentileofscore(read_by_pandas['Rate'], row) for row in read_by_pandas['Rate']])\n",
    "convert_rate_sat = read_by_pandas.assign(Percentiles = new_column)\n",
    "convert_rate_sat[convert_rate_sat['State']=='CA']['Percentiles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">\n",
    "        3. Percentile rank also assign ranks to the values in a column, which the ranks can be used to calculate for spearman rank correlation. \n",
    "    </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Percentiles and outliers\n",
    "\n",
    "1. Why might percentile scoring be useful for dealing with outliers?\n",
    "2. Plot the distribution of a variable of your choice from the drug use dataset.\n",
    "3. Plot the same variable but percentile scored.\n",
    "4. Describe the effect, visually, of coverting raw scores to percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">\n",
    "        1. It is useful because outliers tend to skew the graph to a large extent. By using percentile scoring, this skewness will be greatly reduced when we only consider the distance between 2 percentiles and not the distance between their absolute values.\n",
    "    </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2. Plot the distribution of a variable of your choice from the drug use dataset')\n",
    "variable_choice = drug_use_df[['alcohol-use']]\n",
    "plt.scatter(variable_choice.index, variable_choice);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_variable = pd.DataFrame([stats.percentileofscore(variable_choice, row) for row in variable_choice.squeeze()])\n",
    "print('3. Plot the same variable but percentile scored.')\n",
    "plt.scatter(scored_variable.index, scored_variable);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#333;color:white;padding:10px\">\n",
    "    <h3 style=\"margin:10px\">\n",
    "4. After converting to percentile, the graph appears to be more linear prior to the conversion. The maximum value has been converted to 100.\n",
    "    </h3>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
