{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=[YOUR_CHROME_DRIVER_HERE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at keyword: Machine Learning\n",
      "Scraping jobs start for 339 entries...\n",
      "Job #10\n",
      "Job #20\n",
      "Job #30\n",
      "Job #40\n",
      "Job #50\n",
      "Job #60\n",
      "Job #70\n",
      "Job #80\n",
      "Job #90\n",
      "Job #100\n",
      "Job #110\n",
      "Job #120\n",
      "Job #130\n",
      "Job #140\n",
      "Job #150\n",
      "Job #160\n",
      "Job #170\n",
      "Job #180\n",
      "Job #190\n",
      "Job #200\n",
      "Job #210\n",
      "Job #220\n",
      "Job #230\n",
      "Job #240\n",
      "Job #250\n",
      "Job #260\n",
      "Job #270\n",
      "Job #280\n",
      "Job #290\n",
      "Job #300\n",
      "Job #310\n",
      "Job #320\n",
      "Job #330\n",
      "Done scraping 339 entries!\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "keywords = ['Machine Learning', 'Research Scientist', 'Business Intelligence', 'Data Engineer', 'Tableau', 'deep learning', 'computer vision', 'tensorflow']\n",
    "for keyword in keywords:\n",
    "    print('Looking at keyword:', keyword)\n",
    "    links = pd.read_csv(keyword + ' job links.csv', header=None, names=['url'])\n",
    "    print('Scraping jobs start for', links.shape[0], 'entries...')\n",
    "    for i in links['url']:\n",
    "        try:\n",
    "            counter += 1\n",
    "            if (counter % 10 == 0):\n",
    "                print('Job #' + str(counter))\n",
    "            driver.get(i)\n",
    "            sleep(10)\n",
    "            source = driver.page_source\n",
    "\n",
    "            folder_path = './source_code/' + keyword\n",
    "            if not os.path.isdir(folder_path):\n",
    "                os.mkdir(folder_path)\n",
    "\n",
    "            filename = i.split('/')[-1]\n",
    "            with gzip.open(folder_path + '/' + filename + '.txt.gz', 'wb') as f:\n",
    "                f.write(source.encode())\n",
    "#             sleep(0.5)\n",
    "        except FileNotFoundError:\n",
    "            print('Error while saving for this URL:', i)\n",
    "            continue\n",
    "    print('Done scraping', links.shape[0], 'entries!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
